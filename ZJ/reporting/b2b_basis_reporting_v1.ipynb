{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PhilippPascalReicher\\AppData\\Local\\Temp\\ipykernel_4200\\3248813834.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import sqlalchemy \n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.sql import text\n",
    "import psycopg2 \n",
    "import io\n",
    "import datetime as dt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "with open(os.getenv('USER_PATH'), 'r') as f:\n",
    "    config = json.load(f)\n",
    "import numpy as np\n",
    "\n",
    "config['database'] = config['database'] \n",
    "\n",
    "### engines = {}\n",
    "### for x in ['mkt', 'dwh_db']:\n",
    "###    engines[x] = sqlalchemy.create_engine('postgresql://'+config['user']+':'+config['password']+'@'+config['host']+':'+str(config['port'])+f'/{x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect with Matomo API\n",
    "import requests\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Define your Matomo server and API token\n",
    "matomo_server = os.getenv('MATOMO_REQUEST_URL')\n",
    "api_token = os.getenv('MATOMO_API_KEY')\n",
    "date = '2024-03-01,2024-03-15'\n",
    "\n",
    "# Define the API endpoint\n",
    "url = f'{matomo_server}/index.php?module=API&method=API.get&format=JSON&idSite=1&period=day&date={date}&token_auth={api_token}'\n",
    "\n",
    "# Make the request\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    # Create an empty list to store reshaped data\n",
    "    reshaped_data = []\n",
    "    # Iterate through each date in the JSON response\n",
    "    for date, values in data.items():\n",
    "        # Create a new dictionary for each date\n",
    "        date_data = {'date': date}\n",
    "        # Add other data values as columns in the dictionary\n",
    "        for key, value in values.items():\n",
    "            date_data[key] = value\n",
    "        # Append the dictionary to the reshaped data list\n",
    "        reshaped_data.append(date_data)\n",
    "    # Convert the reshaped data into a DataFrame\n",
    "    df = pd.DataFrame(reshaped_data)\n",
    "else:\n",
    "    print(\"Error occurred:\", response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting with Segment API\n",
    "import segment.analytics as analytics\n",
    "\n",
    "analytics.write_key = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Segment \n",
    "import segment.analytics as analytics\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "client = analytics.Client(write_key=os.getenv('SEGMENT_WRITE_KEY')) # write key for Segment source: Website - JS - DEV\n",
    "\n",
    "# Define destination in Segment \n",
    "destination = 'Indicative - Website - DEV'\n",
    "\n",
    "# Retrieve data from Segment for the past 7 days\n",
    "start_date = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')\n",
    "end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "# print(dir(client)) # Find available attributes of the client object\n",
    "\n",
    "#Get data and convert to Pandas DataFrame\n",
    "data = client.export(\n",
    "    destination=destination, \n",
    "    format='json',\n",
    "    start=start_date,\n",
    "    end=end_date\n",
    ")\n",
    "#df = pd.json_normalize(data)\n",
    "\n",
    "# Save DataFrame to a CSV file\n",
    "# df.to_csv('segment_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 500 - {\"message\":\"There was an error processing your request. It has been logged (ID 9a0476dd41a405c1).\"}\n"
     ]
    }
   ],
   "source": [
    "# Get data from Indicative User Segment Export API\n",
    "import requests\n",
    "from getpass import getpass\n",
    "\n",
    "# Set your API key and endpoint URL\n",
    "api_key = os.getenv('MPARTICLE_ANALYTICS_API_KEY')\n",
    "endpoint_url = os.getenv('MPARTICLE_ANALYTICS_REQUEST_URL')\n",
    "\n",
    "# Make a GET request to retrieve data\n",
    "headers = {'Authorization': f'Bearer {api_key}'}\n",
    "response = requests.get(endpoint_url, headers=headers)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    data = response.json()  # Parse the JSON response\n",
    "    # Process the data as needed (e.g., save to a file, analyze, etc.)\n",
    "    print(\"Data retrieved successfully!\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code} - {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting with Salesforce API\n",
    "from simple_salesforce import Salesforce\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "sf = Salesforce(username='philipp.reichert@zenjob.com',password='vgr1pem!edt!wtg1JCK', security_token='6kDHQsmxM3bTjaF1gArpYHSZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Salesforce report data\n",
    "sf_instance = os.getenv('SALESFORCE_INSTANCE_URL') # Using my.salesforce instead of lightning.force \n",
    "reportId = '00O09000009zxlFEAQ' \n",
    "export = '?isdtp=p1&export=1&enc=UTF-8&xf=csv'\n",
    "sfUrl = sf_instance + reportId + export\n",
    "response = requests.get(sfUrl, headers=sf.headers, cookies={'sid': sf.session_id})\n",
    "download_report = response.content.decode('utf-8')\n",
    "df1 = pd.read_csv(StringIO(download_report))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dwh connection\n",
    "dwh_engine = sqlalchemy.create_engine('postgresql://'+config['user']+':'+config['password']+'@'+config['host']+':'+str(config['port'])+'/'+config['database'])\n",
    "### engines = sqlalchemy.create_engine('postgresql://'+config['user']+':'+config['password']+'@'+config['host']+':'+str(config['port'])+'/[database]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'C'\n",
    "from google.analytics.data_v1beta import BetaAnalyticsDataClient\n",
    "from google.analytics.data_v1beta.types import DateRange\n",
    "from google.analytics.data_v1beta.types import Dimension\n",
    "from google.analytics.data_v1beta.types import Metric\n",
    "from google.analytics.data_v1beta.types import FilterExpression\n",
    "from google.analytics.data_v1beta.types import Filter\n",
    "from google.analytics.data_v1beta.types import RunReportRequest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "start_date = dt.date(2023,1,2) \n",
    "end_date = dt.datetime.now().date() + dt.timedelta(days=-(dt.datetime.now().weekday() + 1))\n",
    "# which paid campaigns\n",
    "campaign_name_strings_include = ['b2b']\n",
    "campaign_name_strings_exclude = ['b2c']\n",
    "# which website pages\n",
    "b2b_website_pages = ['de/personal', 'en/companies', 'de/erfolgsgeschichten', 'de/ressourcen'] ### add additional b2b pages here\n",
    "# how many days per pull\n",
    "batch_size_days = 10\n",
    "# how many lines per write\n",
    "batch_size_write_lines = 1000\n",
    "# target table\n",
    "target_table = {\n",
    "    'db':'mkt',\n",
    "    'schema':'mkt',\n",
    "    'table':'mkt_b2b_basis_reporting'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to interact with GA4\n",
    "def ga4_response_to_df(response):\n",
    "    dim_len = len(response.dimension_headers)\n",
    "    metric_len = len(response.metric_headers)\n",
    "    all_data = []\n",
    "    for row in response.rows:\n",
    "        row_data = {}\n",
    "        for i in range(0, dim_len):\n",
    "            row_data.update({response.dimension_headers[i].name: row.dimension_values[i].value})\n",
    "        for i in range(0, metric_len):\n",
    "            row_data.update({response.metric_headers[i].name: row.metric_values[i].value})\n",
    "        all_data.append(row_data)\n",
    "    df = pd.DataFrame(all_data)\n",
    "    return df\n",
    "\n",
    "def run_report_df(property_id, dimensions, metrics, start_date, end_date, limit, offset):\n",
    "    dimensions_ga4 = []\n",
    "    for dimension in dimensions:\n",
    "        dimensions_ga4.append(Dimension(name=dimension))\n",
    "    metrics_ga4 = []\n",
    "    for metric in metrics:\n",
    "        metrics_ga4.append(Metric(name=metric))\n",
    "    client = BetaAnalyticsDataClient()\n",
    "\n",
    "    request = RunReportRequest(\n",
    "        property=f\"properties/{property_id}\",\n",
    "        dimensions=dimensions_ga4,\n",
    "        metrics=metrics_ga4,\n",
    "        offset=offset,\n",
    "        limit=limit,\n",
    "        date_ranges=[DateRange(start_date=start_date.strftime('%Y-%m-%d'),\n",
    "                                end_date=end_date.strftime('%Y-%m-%d'))])\n",
    "\n",
    "    response = client.run_report(request)\n",
    "    return ga4_response_to_df(response)\n",
    "\n",
    "def auto_paginate(property_id, dimensions, metrics, start_date, end_date):\n",
    "    limit = 100000\n",
    "    offset = 0\n",
    "    i = 0\n",
    "\n",
    "    # first run\n",
    "    tmp = pd.DataFrame()\n",
    "    tmp = run_report_df(property_id, dimensions, metrics, start_date, end_date, limit, offset)\n",
    "    i = i + 1\n",
    "\n",
    "    while len(tmp) == (i * limit):\n",
    "        offset = (i * limit)\n",
    "        response = run_report_df(property_id, dimensions, metrics, start_date, end_date, limit, offset)\n",
    "        tmp = pd.concat([\n",
    "            response,\n",
    "            tmp\n",
    "        ])\n",
    "        i = i + 1\n",
    "    # clear up this horrible date format\n",
    "    try: # use try because sometimes we don't have a date. \n",
    "        tmp['date'] = pd.to_datetime(tmp['date'], format='%Y%m%d')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # make metrics useable\n",
    "    for x in metrics:\n",
    "        tmp[x] = tmp[x].astype('float')\n",
    "    \n",
    "    return tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get impacted B2B campaigns\n",
    "strings_include_str = ' OR '.join([f\"(UPPER(campaign) LIKE '%%{y.upper()}%%')\" for y in campaign_name_strings_include])\n",
    "strings_exclude_str = ' OR '.join([f\"(UPPER(campaign) NOT LIKE '%%{y.upper()}%%')\" for y in campaign_name_strings_exclude])\n",
    "q = f\"SELECT DISTINCT campaign FROM cds.fact_consolidated_channel_marketing_cost WHERE ({strings_include_str}) AND ({strings_exclude_str}) AND date_dt BETWEEN '{start_date}' AND '{end_date}';\"\n",
    "impacted_campaigns = pd.read_sql(q, engines)\n",
    "impacted_campaigns = [y for y in impacted_campaigns['campaign']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get spend impr clicks for impacted campaigns\n",
    "dates = [y.date() for y in pd.date_range(start=start_date, end=end_date)]\n",
    "date_batches = []\n",
    "for x in range(0,len(dates),batch_size_days):\n",
    "    date_batches.append(dates[x:x+batch_size_days])\n",
    "campaign_where_string = ','.join([f\"'{y}'\" for y in impacted_campaigns])\n",
    "campaign_metrics = []\n",
    "for x in tqdm(date_batches):\n",
    "    q = f'''SELECT \n",
    "        *\n",
    "    FROM\n",
    "        cds.fact_consolidated_channel_marketing_cost\n",
    "    WHERE\n",
    "        date_dt BETWEEN '{min(x)}' AND '{max(x)}'\n",
    "        AND campaign IN ({campaign_where_string});'''\n",
    "    tmp = pd.read_sql(q, engines['dwh_db'])\n",
    "    campaign_metrics.append(tmp)\n",
    "campaign_metrics = pd.concat(campaign_metrics).reset_index(drop=True)\n",
    "campaign_metrics = campaign_metrics[[y for y in campaign_metrics if y != 'cpc']]\n",
    "campaign_metrics['date_dt'] = pd.to_datetime(campaign_metrics['date_dt']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get b2b events\n",
    "ga4_events = auto_paginate(271650907, ['date', 'eventName'], ['activeUsers', 'sessions', 'eventCount'], start_date, end_date)\n",
    "ga4_events = ga4_events[ga4_events['eventName'].str.contains('b2b', case=False)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get b2b pages\n",
    "ga4_pages = auto_paginate(271650907, ['date', 'sessionSourceMedium', 'pagePath'], ['activeUsers', 'sessions', 'screenPageViews'], start_date, end_date)\n",
    "b2b_website_pages_pagePaths = []\n",
    "for x in b2b_website_pages:\n",
    "    b2b_website_pages_pagePaths = b2b_website_pages_pagePaths + [y for y in ga4_pages['pagePath'] if x.upper() in y.upper()]\n",
    "b2b_website_pages_pagePaths = list(set(b2b_website_pages_pagePaths))\n",
    "ga4_pages = ga4_pages[ga4_pages['pagePath'].isin(b2b_website_pages_pagePaths)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delim = ' / '\n",
    "ga4_pages.loc[ga4_pages['sessionSourceMedium'] == '(not set)', 'sessionSourceMedium'] = delim.join(['(not set)','(not set)'])\n",
    "ga4_pages['sessionSource'] = [y[0] for y in ga4_pages['sessionSourceMedium'].str.split(delim)]\n",
    "ga4_pages['sessionMedium'] = [y[1] for y in ga4_pages['sessionSourceMedium'].str.split(delim)]\n",
    "ga4_pages.loc[ga4_pages['sessionSource'] == '(direct)', 'web_origin_type'] = 'Organic'\n",
    "ga4_pages.loc[ga4_pages['sessionMedium'] == 'organic', 'web_origin_type'] = 'Organic'\n",
    "ga4_pages.loc[ga4_pages['sessionSourceMedium'] == '(not set)', 'web_origin_type'] = 'Organic'\n",
    "ga4_pages.loc[ga4_pages['web_origin_type'].isna(), 'web_origin_type'] = 'Paid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get organic metrics\n",
    "organic_where_string = ' OR '.join([f\"page LIKE '%%{y}%%'\" for y in b2b_website_pages])\n",
    "organic_metrics = []\n",
    "for x in tqdm(date_batches):\n",
    "    q = f'''SELECT \n",
    "        *\n",
    "    FROM\n",
    "        cds.fact_organic_search\n",
    "    WHERE\n",
    "        \"date\" BETWEEN '{min(x)}' AND '{max(x)}'\n",
    "        AND ({organic_where_string});'''\n",
    "    tmp = pd.read_sql(q, engines['dwh_db'])\n",
    "    organic_metrics.append(tmp)\n",
    "organic_metrics = pd.concat(organic_metrics).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ga4_pages.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(\n",
    "    [\n",
    "        ga4_events.groupby(['date','eventName'], as_index=False).agg({'eventCount':'sum'}).rename(columns={'date':'date_dt'}),\n",
    "        organic_metrics.groupby(['date'], as_index=False).agg({y:'sum' for y in ['impressions', 'clicks']}).rename(columns={y:'b2b_organic_'+y for y in ['impressions', 'clicks']}).rename(columns={'date':'date_dt'}),\n",
    "        campaign_metrics.groupby(['date_dt'], as_index=False).agg({y:'sum' for y in ['ad_cost', 'impressions', 'clicks']}).rename(columns={\n",
    "        'ad_cost':'b2b_spend',\n",
    "        'impressions':'b2b_paid_impr',\n",
    "        'clicks':'b2b_paid_clicks'\n",
    "        }),\n",
    "        ga4_pages.groupby(['date', 'web_origin_type'], as_index=False).agg({'screenPageViews':'sum'}).rename(columns={'date':'date_dt', 'screenPageViews':'ga4_pageviews'})\n",
    "    ]\n",
    ")\n",
    "df['date_dt'] = pd.to_datetime(df['date_dt']).dt.date\n",
    "# dims should go at the front so we don't look like lunatics\n",
    "dims = ['date_dt', 'eventName', 'web_origin_type']\n",
    "metrics = [y for y in df.columns if (y not in dims)]\n",
    "df = df[dims + metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks_to_write = []\n",
    "for x in range(0,len([y for y in df.index]), batch_size_write_lines):\n",
    "    blocks_to_write.append(df[x:x+batch_size_write_lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create table \n",
    "df.head(0).to_sql(target_table['table'], engines[target_table['db']], schema=target_table['schema'], if_exists='replace',index=False, method='multi',chunksize=10000)\n",
    "for x in tqdm(blocks_to_write):\n",
    "    x.to_sql(target_table['table'], engines[target_table['db']], schema=target_table['schema'], if_exists='append',index=False, method='multi',chunksize=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['b2b_spend'].sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
